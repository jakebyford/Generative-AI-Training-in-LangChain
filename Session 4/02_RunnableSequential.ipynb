{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b28d6e8",
   "metadata": {},
   "source": [
    "## 📌 Understanding `RunnableSequence` in LangChain\n",
    "\n",
    "### **What is `RunnableSequence`?**\n",
    "✅ `RunnableSequence` is a **workflow builder** that **chains multiple steps together** in a sequence.  \n",
    "✅ It allows **LLMs, embeddings, retrievers, functions, and tools** to work **sequentially** in LangChain.  \n",
    "✅ Works as a **replacement for `SequentialChain`**, which is now deprecated.\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Why Use `RunnableSequence`?**\n",
    "✔ **Standardized execution** → Works with LLMs, retrievers, and Python functions.  \n",
    "✔ **Preserves data flow** → Each step's output becomes the next step’s input.  \n",
    "✔ **More flexibility than `SequentialChain`** → Supports **parallel processing, async execution, and streaming**.\n",
    "\n",
    "- https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableSequence.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read api_key from file\n",
    "with open('../api_keys.txt', 'r') as file:\n",
    "    api_key = file.read()\n",
    "\n",
    "\n",
    "# Set loaded api_key as OPENAI_API_KEY environmental variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ae7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( (\"system\", \"Summarize the following {text} in 2 bullet points\") )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97110748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The resulting RunnableSequence is a runnable that can be invoked, streamed, or further chained like any other runnable.\n",
      "- Chaining runnables in this way allows for efficient streaming of output and enables debugging and tracing with tools like LangSmith.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "# Create a sequential workflow\n",
    "seq_chain = RunnableSequence(\n",
    "    prompt,\n",
    "    llm\n",
    ")\n",
    "\n",
    "\n",
    "output = seq_chain.invoke({\"text\": \"The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like LangSmith.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd365c",
   "metadata": {},
   "source": [
    "### ☕️ Or simpler Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e06777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The RunnableSequence is a runnable that can be invoked, streamed, or further chained like any other runnable.\n",
      "- Chaining runnables in this way allows for efficient streaming of output and enables debugging and tracing with tools like LangSmith.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "output = chain.invoke({\"text\": \"The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like LangSmith.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f658ac",
   "metadata": {},
   "source": [
    "## **What is `RunnableLambda`?**\n",
    "`RunnableLambda` is a **lightweight wrapper** that allows **any Python function** to be used as a **step in a LangChain pipeline**. It helps in **data transformations, preprocessing, and custom logic** between AI model interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Why Use `RunnableLambda`?**\n",
    "✅ **Makes Python functions compatible** with LangChain's `Runnable` framework.  \n",
    "✅ **Can be used in AI workflows** for **text transformation, data filtering, logging, and conditional branching**.  \n",
    "✅ **Works in sequence with LLMs, retrievers, or databases**.\n",
    "\n",
    "- https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc3601",
   "metadata": {},
   "source": [
    "### **👀 Example 1: Uppercase Runnable**\n",
    "🔹 **Scenario:** Convert input to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b313be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Define a simple function\n",
    "def to_uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "# Wrap it in a RunnableLambda\n",
    "uppercase_runnable = RunnableLambda(to_uppercase) \n",
    "\n",
    "\n",
    "\n",
    "# Execute it\n",
    "output = uppercase_runnable.invoke(\"hello langchain\")\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66517fa3",
   "metadata": {},
   "source": [
    "### **👀 Example 2: Text Processing Pipeline**\n",
    "🔹 **Scenario:** Convert input to uppercase → Reverse text → Count words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3da7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Step 1: Convert text to uppercase\n",
    "def to_uppercase(data: dict) -> dict:\n",
    "    return {\"text\": data[\"text\"].upper()}\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Reverse the text\n",
    "def reverse_text(data: dict) -> dict:\n",
    "    return {\"text\": data[\"text\"][::-1]}\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Count words\n",
    "def count_words(data: dict) -> dict:\n",
    "    return {\"word_count\": len(data[\"text\"].split())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc065a3c",
   "metadata": {},
   "source": [
    "### 🖨️ Printing all middle stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73ffe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_runnable(data: dict) -> dict:\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90dd7756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'HELLO DEAR LANGCHAIN!'}\n",
      "{'text': '!NIAHCGNAL RAED OLLEH'}\n",
      "{'word_count': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential workflow\n",
    "chain = RunnableSequence(\n",
    "    RunnableLambda(to_uppercase) ,\n",
    "    RunnableLambda(print_runnable) ,\n",
    "    RunnableLambda(reverse_text) ,\n",
    "    RunnableLambda(print_runnable) ,\n",
    "    RunnableLambda(count_words)\n",
    ")\n",
    "\n",
    "\n",
    "output = chain.invoke({\"text\": \"Hello dear LangChain!\"})\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ac20f",
   "metadata": {},
   "source": [
    "### ➿ Using `|` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 3}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableLambda(to_uppercase) | RunnableLambda(reverse_text) | RunnableLambda(count_words)\n",
    "output = chain.invoke({\"text\": \"Hello dear LangChain!\"})\n",
    "print(output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59776a",
   "metadata": {},
   "source": [
    "### ➿ Using `Lambda fucntion` on multiple lines\n",
    "- Don't forget to put all the steps within `paranthesis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3bc80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 3}\n"
     ]
    }
   ],
   "source": [
    "chain = (RunnableLambda(to_uppercase) \n",
    "        | RunnableLambda(reverse_text) \n",
    "        | RunnableLambda(count_words)\n",
    "        )\n",
    "\n",
    "\n",
    "output = chain.invoke({\"text\": \"Hello dear LangChain!\"})\n",
    "print(output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ef7c4",
   "metadata": {},
   "source": [
    "### ➿ Doing the same but in one-liner `lambda` functions\n",
    "- You can return of each `lambda` fucntion as a `dictionary` or a `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b373dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'HELLO LANGCHAIN!'}\n",
      "{'text': '!NIAHCGNAL OLLEH'}\n",
      "{'count': 2}\n"
     ]
    }
   ],
   "source": [
    "runnbale_upper_case = RunnableLambda(lambda x: {'text': x['text'].upper()} ) \n",
    "runnbale_reverse = RunnableLambda(lambda x: {'text': x['text'][::-1]} ) \n",
    "runnabel_count_words = RunnableLambda(lambda x: {'count': len(x['text'].split())} )\n",
    "\n",
    "\n",
    "chain = runnbale_upper_case  | print_runnable | runnbale_reverse | print_runnable |runnabel_count_words\n",
    "\n",
    "\n",
    "\n",
    "output = chain.invoke({\"text\": \"Hello LangChain!\"})\n",
    "print(output)  # Output: {'word_count': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f885ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO LANGCHAIN!\n",
      "!NIAHCGNAL OLLEH\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "runnbale_upper_case = RunnableLambda(lambda x: x['text'].upper()) \n",
    "runnbale_reverse = RunnableLambda(lambda x: x[::-1] ) \n",
    "runnabel_count_words = RunnableLambda(lambda x: len(x.split()) )\n",
    "\n",
    "\n",
    "chain = runnbale_upper_case  | print_runnable | runnbale_reverse | print_runnable |runnabel_count_words\n",
    "\n",
    "\n",
    "\n",
    "output = chain.invoke({\"text\": \"Hello LangChain!\"})\n",
    "print(output)  # Output: {'word_count': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130197d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d241d4",
   "metadata": {},
   "source": [
    "## 👀 Example 3: Using `RunnableSequence` with LLMs and **chaining chains**\n",
    "\n",
    "1. `summary_prompt`: **Summarizes an input document.**  \n",
    "2. `topics_chain`: **Extracts key topics from the summary.**  \n",
    "3. `questions_chain`: **Creates follow-up questions for deeper analysis.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9dfb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
    "\n",
    "\n",
    "# Load LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b750c3",
   "metadata": {},
   "source": [
    "### 🧱 Step 1: Generate Summary Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Summarize the following text:\\nLangChain simplifies building AI-powered applications with modular components.')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Generate Summary\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"Summarize the following text:\\n{text}\")\n",
    "\n",
    "summary_prompt.invoke({\"text\": \"LangChain simplifies building AI-powered applications with modular components.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af538de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain streamlines the development of AI applications by providing modular components.', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 26, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-868052a9-585d-40fa-97a1-bafc853d87fe-0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain = summary_prompt | llm\n",
    "\n",
    "summary = summary_chain.invoke({\"text\": \"LangChain simplifies building AI-powered applications with modular components.\"})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4262479",
   "metadata": {},
   "source": [
    "### 🧱 Step 2: Generate Topics chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b4b874a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- LangChain\\n- AI applications\\n- Modular components\\n- Development\\n- Streamlining', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 175, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c4b6a78c-6e3a-42a2-a7ea-f4b488807ffd-0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "topics_prompt = ChatPromptTemplate.from_template(\"Extract key topics from this summary:\\n{summary}\")\n",
    "\n",
    "topics_chain = topics_prompt | llm \n",
    "\n",
    "topics = topics_chain.invoke({\"summary\": summary})\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb93ec5",
   "metadata": {},
   "source": [
    "### 🧱 Step 3: Generate question chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60225cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Can you provide more details about LangChain and its specific features?\\n2. How are AI applications being integrated with LangChain?\\n3. In what ways are modular components being used in development with LangChain?\\n4. What are some strategies for streamlining the development process when working with LangChain?\\n5. Can you elaborate on the different phases of development involved in implementing LangChain?\\n6. How does LangChain facilitate the creation of AI applications?\\n7. What are the advantages of using modular components in the context of LangChain?\\n8. Are there any specific challenges that developers may face when streamlining the development process with LangChain?\\n9. How does LangChain support collaboration and integration of different components in development?\\n10. What are some best practices for optimizing the performance of AI applications developed using LangChain?', response_metadata={'token_usage': {'completion_tokens': 165, 'prompt_tokens': 184, 'total_tokens': 349, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-56941194-b08e-42ce-963e-4873de1204a1-0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Generate Follow-Up Questions\n",
    "questions_prompt = ChatPromptTemplate.from_template(\"Generate follow-up questions for these topics:\\n{topics}\")\n",
    "\n",
    "questions_chain = questions_prompt | llm\n",
    "\n",
    "questions_chain.invoke({\"topics\": topics})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb54b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can you explain more about what LangChain is and how it works?\n",
      "2. What are some specific examples of AI applications that utilize LangChain?\n",
      "3. How do modular components play a role in the development of LangChain?\n",
      "4. In what ways does LangChain make the creation process easier compared to traditional methods?\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    # Step 1: First Summarization and the pass {\"summary\": summary from content of previous stage} to the next stage\n",
    "    summary_chain  # >>> summerize the text, output has content variable\n",
    "      |  \n",
    "    # Step 2: Extract topics and pass {\"topics\": Extract key topics from the pevious summary} to the next stage\n",
    "    topics_chain # Extract topics from the previous summary, output has content variabl\n",
    "      |  \n",
    "    # Step 3: Generate questions\n",
    "    questions_chain  # Output has content variable\n",
    ")\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "output = chain.invoke({\"text\": \"LangChain simplifies building AI-powered applications with modular components.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f2cd7",
   "metadata": {},
   "source": [
    "## 🕵️ How to inspect runnables\n",
    "- https://python.langchain.com/docs/how_to/inspect/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8d9ca",
   "metadata": {},
   "source": [
    "`!pip install grandalf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15316cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+    \n",
      "    | PromptInput |    \n",
      "    +-------------+    \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| ChatPromptTemplate | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "    +------------+     \n",
      "    | ChatOpenAI |     \n",
      "    +------------+     \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      " +------------------+  \n",
      " | ChatOpenAIOutput |  \n",
      " +------------------+  \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| ChatPromptTemplate | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "    +------------+     \n",
      "    | ChatOpenAI |     \n",
      "    +------------+     \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      " +------------------+  \n",
      " | ChatOpenAIOutput |  \n",
      " +------------------+  \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| ChatPromptTemplate | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "    +------------+     \n",
      "    | ChatOpenAI |     \n",
      "    +------------+     \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      " +------------------+  \n",
      " | ChatOpenAIOutput |  \n",
      " +------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d3c4626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='Summarize the following text:\\n{text}'))]),\n",
       " ChatPromptTemplate(input_variables=['summary'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['summary'], template='Extract key topics from this summary:\\n{summary}'))]),\n",
       " ChatPromptTemplate(input_variables=['topics'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topics'], template='Generate follow-up questions for these topics:\\n{topics}'))])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ebfc6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
