{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **üîßFunctions:** \n",
    "LangChain provides powerful helper methods to extend the functionality of **Runnables** in LCEL pipelines. Below is a breakdown of the most useful ones with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîπ What is `.assign()` in LangChain?**\n",
    "‚úÖ `.assign()` is a **helper method** that allows you to **add new computed fields** to an existing dictionary **without modifying the original input**.  \n",
    "‚úÖ It is often used with **`RunnablePassthrough`** to **pass through existing data** while adding additional fields.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ How `.assign()` Works**\n",
    "### **General Syntax**\n",
    "```python\n",
    "RunnablePassthrough().assign(new_key=lambda inputs: some_computation(inputs))\n",
    "```\n",
    "\n",
    "### `.assign()` expects a dictionary as an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üëÄ Example 1: Adding a Word Count to Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'LangChain is awesome!'}\n",
      "{'text': 'LangChain is awesome!', 'word_count': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "pipeline = RunnablePassthrough()\n",
    "\n",
    "# Invoke the pipeline\n",
    "output = pipeline.invoke({\"text\": \"LangChain is awesome!\"})\n",
    "print(output)\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "# Create a passthrough pipeline and assign a new field\n",
    "pipeline = RunnablePassthrough().assign(\n",
    "    word_count=lambda inputs: len(inputs[\"text\"].split())\n",
    ")\n",
    "\n",
    "# Invoke the pipeline\n",
    "output = pipeline.invoke({\"text\": \"LangChain is awesome!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üëÄ Example 2: Combining `.assign()` with LLMs**\n",
    "**Scenario:**\n",
    "- Run an LLM to generate text.\n",
    "- Assign a field to store the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read api_key from file\n",
    "with open('../api_keys.txt', 'r') as file:\n",
    "    api_key = file.read()\n",
    "\n",
    "\n",
    "# Set loaded api_key as OPENAI_API_KEY environmental variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Quantum computing uses quantum bits (qubits) to perform complex calculations exponentially faster than classical computers, revolutionizing data processing.', 'length': 156}\n"
     ]
    }
   ],
   "source": [
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain {topic} in 20 words.\")\n",
    "\n",
    "\n",
    "# Convert LLM output to a dictionary so `.assign()` can process it\n",
    "convert_to_dict_runnable = RunnableLambda(lambda x: {\"content\": x.content})\n",
    "\n",
    "\n",
    "# Create pipeline with `.assign()`\n",
    "pipeline = (prompt | llm | convert_to_dict_runnable).assign(\n",
    "    length=lambda inputs: len(inputs[\"content\"])\n",
    ")\n",
    "\n",
    "# Invoke the pipeline\n",
    "output = pipeline.invoke({\"topic\": \"quantum computing\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üëÄExample 3: Combining `.assign()` with Parallel Execution**\n",
    "**Scenario:**\n",
    "- Run two different LLM chains in parallel.\n",
    "- Assign a new field that counts total characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry_response': \"What do you want? Can't you see I'm busy?\", 'happy_response': \"Hello! I hope you're having a wonderful day.\", 'total_chars': 85}\n"
     ]
    }
   ],
   "source": [
    "# Correctly define prompts using from_messages()\n",
    "angry_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an angry person. Respond to: {input}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "happy_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a happy person. Respond to: {input}\")\n",
    "])\n",
    "\n",
    "\n",
    "# Define a parallel runnable\n",
    "runnable = RunnableParallel(\n",
    "                        angry_response = angry_prompt | llm | RunnableLambda(lambda x: x.content),\n",
    "                        happy_response = happy_prompt | llm  | RunnableLambda(lambda x: x.content)\n",
    "        ) | RunnablePassthrough.assign( # assigning a new field to the output dictionary\n",
    "                                total_chars=lambda inputs: len(inputs['angry_response'] + inputs['happy_response'])\n",
    "                                )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Invoke the pipeline\n",
    "output = runnable.invoke({\"input\": \"hello\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚òïÔ∏è Same Logic, Different Syntax\n",
    "\n",
    "\n",
    "```python \n",
    "runnable = {\n",
    "    'angry_response':  angry_prompt | llm | RunnableLambda(lambda x: x.content),\n",
    "    'happy_response':  appy_prompt | llm  | RunnableLambda(lambda x: x.content),\n",
    "} \n",
    "``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attention!**\n",
    "You can achieve the same result using `RunnableLambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry_response': \"What do you want? Don't waste my time with meaningless greetings.\", 'happy_response': 'Hello! I hope you are doing well today. How can I make your day even brighter?', 'total_chars': 143}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Correctly define prompts using from_messages()\n",
    "angry_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an angry person. Respond to: {input}\")\n",
    "])\n",
    "\n",
    "happy_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a happy person. Respond to: {input}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Define a RunnableLambda that calculates total characters\n",
    "calculate_total_chars = RunnableLambda(lambda inputs: {\n",
    "    **inputs,  # Keep the original dictionary keys and values ({angry_response: ... ,happy_response: ... })\n",
    "    'total_chars': len(inputs['angry_response'] + inputs['happy_response'])  # Compute total character count\n",
    "})\n",
    "\n",
    "\n",
    "# Define a parallel runnable\n",
    "runnable = RunnableParallel(\n",
    "    angry_response=angry_prompt | llm | RunnableLambda(lambda x: x.content),\n",
    "    happy_response=happy_prompt | llm  | RunnableLambda(lambda x: x.content)\n",
    ") | calculate_total_chars  # Apply the RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the pipeline\n",
    "output = runnable.invoke({\"input\": \"hello\"})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   +----------------------------------------------+    \n",
      "   | Parallel<angry_response,happy_response>Input |    \n",
      "   +----------------------------------------------+    \n",
      "                  ***             ***                  \n",
      "                **                   **                \n",
      "              **                       **              \n",
      "+--------------------+          +--------------------+ \n",
      "| ChatPromptTemplate |          | ChatPromptTemplate | \n",
      "+--------------------+          +--------------------+ \n",
      "           *                               *           \n",
      "           *                               *           \n",
      "           *                               *           \n",
      "    +------------+                  +------------+     \n",
      "    | ChatOpenAI |                  | ChatOpenAI |     \n",
      "    +------------+                  +------------+     \n",
      "           *                               *           \n",
      "           *                               *           \n",
      "           *                               *           \n",
      "      +--------+                      +--------+       \n",
      "      | Lambda |                      | Lambda |       \n",
      "      +--------+**                   *+--------+       \n",
      "                  ***             ***                  \n",
      "                     **         **                     \n",
      "                       **     **                       \n",
      "  +-----------------------------------------------+    \n",
      "  | Parallel<angry_response,happy_response>Output |    \n",
      "  +-----------------------------------------------+    \n",
      "                           *                           \n",
      "                           *                           \n",
      "                           *                           \n",
      "                      +--------+                       \n",
      "                      | Lambda |                       \n",
      "                      +--------+                       \n",
      "                           *                           \n",
      "                           *                           \n",
      "                           *                           \n",
      "                   +--------------+                    \n",
      "                   | LambdaOutput |                    \n",
      "                   +--------------+                    \n"
     ]
    }
   ],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare `assign()` in `RunnableParallel` and `RunnableSequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_hundred(input):\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'hello', 'input2': 'goodbye', 'from_assign': 100}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnablePassthrough().assign(from_assign=RunnableLambda(return_hundred))\n",
    "\n",
    "result = chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from_parallel': {'input': 'hello', 'input2': 'goodbye'}, 'from_assign': 100}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableParallel({\"from_parallel\": RunnablePassthrough()}).assign(from_assign=RunnableLambda(return_hundred))\n",
    "\n",
    "result = chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üëæ Good to know**\n",
    "LangChain automatically treats a dictionary as a `RunnableParallel` only when it's used inside a pipeline (e.g., `|` pipe). But if you're working with it standalone (like trying to call `.assign()` on it), you must explicitly wrap it.\n",
    "\n",
    "`chain = {\"from_parallel\": RunnablePassthrough()}.assign(from_assign=RunnableLambda(return_hundred))` throws an error.\n",
    "\n",
    "**Below is the right way to implicit create a `RunnableParallel` via Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from_parallel': {'input': 'hello', 'input2': 'goodbye'}, 'from_assign': 100}\n"
     ]
    }
   ],
   "source": [
    "# Single key dictionary interpreted as parallel runnable\n",
    "chain = {\n",
    "    \"from_parallel\": RunnablePassthrough()\n",
    "} | RunnablePassthrough().assign(from_assign=RunnableLambda(return_hundred))\n",
    "\n",
    "\n",
    "\n",
    "result = chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß What is `.bind()` in LangChain?\n",
    "\n",
    "## üìå Definition\n",
    "In LangChain LCEL (LangChain Expression Language), **`.bind()`** is used to **pre-fill fixed values** into a Runnable's expected input.  \n",
    "It‚Äôs like ‚Äúlocking in‚Äù a variable so you don‚Äôt need to provide it every time you call `.invoke()`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What Does `.bind()` Do?\n",
    "\n",
    "- It returns a new `Runnable` with the **specified input(s) already filled in**.\n",
    "- The bound inputs are automatically merged with the ones you provide at runtime.\n",
    "- Great for **customizing reusable chains**, **fixing configuration values**, or **simplifying input**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ **`.bind()` ‚Äî Fix Certain Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sarah, thank you for buying LangChain Pro!\n"
     ]
    }
   ],
   "source": [
    "def render_message(name, product):\n",
    "    return f\"Hello {name}, thank you for buying {product}!\"\n",
    "\n",
    "\n",
    "langchain_thank_you_note = RunnableLambda(render_message).bind(product=\"LangChain Pro\")\n",
    "\n",
    "\n",
    "\n",
    "print(langchain_thank_you_note.invoke(\"Sarah\"))  # Output: \"Hello Sarah, thank you for buying LangChain Pro!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sarah, thank you for buying LangChain Pro!\n",
      "messages=[SystemMessage(content=\"You're a suepr angry and rude customer.\"), HumanMessage(content='respond to this Hello Sarah, thank you for buying LangChain Pro!')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"About time you thanked me, but that's not enough! I demand top-notch customer service and a discount on my next purchase for all the trouble I've had with your product. Get your act together and start treating your customers with the respect they deserve!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a suepr angry and rude customer.\"),\n",
    "    (\"user\", \"respond to this {greeting}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def print_runnable(data: dict) -> dict:\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Bind the system message\n",
    "chain = langchain_thank_you_note  | print_runnable | chat_prompt | print_runnable | llm\n",
    "\n",
    "\n",
    "chain.invoke(\"Sarah\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **‚úÖ`.bind(stop=[, , ,])`**\n",
    "\n",
    "In LangChain LCEL, `.bind(stop=[[, , ,])` is used to **configure generation behavior** for an LLM by specifying **stop sequences** ‚Äî strings that tell the model when to stop generating tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå What Is `stop`?\n",
    "\n",
    "The `stop` parameter is a list of strings.  \n",
    "When the model sees **any of these strings** in the output, it will **immediately stop generating** further text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: x^3 + 7 = 12\n",
      "\n",
      "SOLUTION: \n",
      "Subtract 7 from both sides:\n",
      "x^3 = 5\n",
      "\n",
      "Take the cube root of both sides:\n",
      "x = ‚àõ5\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: x^3 + 7 = 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"Write out the following equation using algebraic symbols then solve it. Use the format\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model.bind(stop=['SOLUTION']) \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß `.bind()` vs `.assign()` in LangChain LCEL\n",
    "\n",
    "| Feature       | `.bind()`                                                  | `.assign()`                                                |\n",
    "|---------------|------------------------------------------------------------|------------------------------------------------------------|\n",
    "| üß† Purpose     | Pre-fill or lock in input values                           | Add new fields to the output dictionary                    |\n",
    "| üéØ Scope       | Affects **inputs before** the Runnable is executed        | Affects **outputs after** the Runnable is executed         |\n",
    "| üß™ Behavior    | Replaces or fills in part of the input                     | Computes extra fields based on the result of a Runnable    |\n",
    "| üß± Input Type  | Typically used before chains or prompts                    | Typically used on output dictionaries                     |\n",
    "| üì• Example     | `chain.bind(topic=\"AI\")`                                   | `chain.assign(length=lambda x: len(x[\"text\"]))`           |\n",
    "| üì§ Output      | Same as original chain, but with fewer required inputs     | Returns original output **plus** new key(s)                |\n",
    "| üîÅ Reusability | Great for creating reusable, pre-configured chains         | Great for enriching outputs for further steps              |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† In Short:\n",
    "\n",
    "- Use **`.bind()`** to **simplify your inputs**.\n",
    "- Use **`.assign()`** to **add data to your outputs**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
