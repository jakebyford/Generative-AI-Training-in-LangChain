{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review**\n",
    "![rev](https://datasciencedojo.com/wp-content/uploads/LLM-Website-blog-thumbnails-61-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üõ†Ô∏è Tools**\n",
    "\n",
    "- The tool abstraction in LangChain associates a Python function with a schema that defines the function's name, description and expected arguments.\n",
    "\n",
    "- Tools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.\n",
    "\n",
    "### **Their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.**\n",
    "\n",
    "- A toolkit is a collection of tools meant to be used together.\n",
    "- https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "\n",
    "Besides the actual function that is called, the Tool consists of several components:\n",
    "\n",
    "- **name**\t`str`\tMust be unique within a set of tools provided to an LLM or agent.\n",
    "- **description**\t`str`\tDescribes what the tool does. Used as context by the LLM or agent.\n",
    "- **return_direct**\t`boolean`\tOnly relevant for agents. When True, after invoking the given tool, the agent will stop and return the result direcly to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Tools vs üßÆ Functions in LangGraph / LangChain\n",
    "\n",
    "| Aspect                        | üîß Tools (`@tool`)                              | üßÆ Functions (Python)                            |\n",
    "|--------------------------------|------------------------------------------------|------------------------------------------------|\n",
    "| Defined with                  | `@tool` decorator or `Tool` class              | Standard Python `def`                          |\n",
    "| Purpose                       | Used by the **LLM itself** (tool-calling)      | Used by **you or LangGraph nodes**             |\n",
    "| Triggered by                  | LLM via `tool_calls`                          | LangGraph edges or manual function call        |\n",
    "| Input format                  | JSON arguments (auto-parsed by LLM)            | Python arguments                               |\n",
    "| Return format                 | Must be `str` (LLM-readable)                  | Any Python object                              |\n",
    "| Metadata                     | Requires name, description, args schema        | Optional (standard code)                      |\n",
    "| Needs registration            | ‚úÖ Yes (via `bind_tools(...)`)                 | ‚ùå No                                           |\n",
    "| Used in LangGraph             | With `ToolNode` or agent-style pattern        | Anywhere as regular `add_node()`               |\n",
    "| Execution trigger             | LLM decides when to call                      | You control when it's called                  |\n",
    "| Example use                   | Calculator, search, database query             | Intermediate logic, routing, computation      |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to create tools**\n",
    "\n",
    "LangChain supports the creation of tools from:\n",
    "\n",
    "\n",
    "\n",
    "- Functions;\n",
    "- LangChain `Runnables`\n",
    "- By sub-classing from `BaseTool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read api_key from file\n",
    "with open(r'C:\\Users\\Seyed Barabadi\\Downloads\\Gen AI\\Incedo Teaching/api_keys.txt', 'r') as file:\n",
    "    api_key = file.read()\n",
    "\n",
    "\n",
    "# Set loaded api_key as OPENAI_API_KEY environmental variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîßFrom Custom Functions**\n",
    "The recommended way to create tools is using the `@tool decorator`\n",
    "\n",
    "- Automatically infer the tool's name, description and expected arguments, while also supporting customization.\n",
    "- Defining tools that return artifacts (e.g. images, dataframes, etc.)\n",
    "- Hiding input arguments from the schema (and hence from the model) using injected tool arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply a and b.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect some of the attributes associated with the tool.\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use the tool directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({'a': 3, 'b':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Multiply a by the maximum of b.',\n",
       " 'properties': {'a': {'description': 'scale factor',\n",
       "   'title': 'A',\n",
       "   'type': 'integer'},\n",
       "  'b': {'description': 'list of ints over which to take maximum',\n",
       "   'items': {'type': 'integer'},\n",
       "   'title': 'B',\n",
       "   'type': 'array'}},\n",
       " 'required': ['a', 'b'],\n",
       " 'title': 'multiply_by_max',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, List\n",
    "\n",
    "\n",
    "# @tool supports parsing of annotations\n",
    "@tool\n",
    "def multiply_by_max(\n",
    "    a: Annotated[int, \"scale factor\"],\n",
    "    b: Annotated[List[int], \"list of ints over which to take maximum\"],\n",
    ") -> int:\n",
    "    \"\"\"Multiply a by the maximum of b.\"\"\"\n",
    "    return a * max(b)\n",
    "\n",
    "\n",
    "\n",
    "multiply_by_max.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_by_max.invoke({'a': 3, 'b':[1, 2 ,3, 4, 5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üèÉ‚Äç‚ôÇÔ∏èFrom Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seyed Barabadi\\AppData\\Local\\Temp\\ipykernel_17676\\164054986.py:20: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  chain_as_tool = chain.as_tool(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='Style responder', description='tool description.', args_schema=<class 'langchain_core.utils.pydantic.PromptInput'>, func=<function convert_runnable_to_tool.<locals>.invoke_wrapper at 0x0000022610A309A0>, coroutine=<function convert_runnable_to_tool.<locals>.ainvoke_wrapper at 0x0000022610A30A40>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.language_models import GenericFakeChatModel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [(\"human\", \"Hello. Please respond in the style of {answer_style}.\")]\n",
    ")\n",
    "\n",
    "\n",
    "# Placeholder LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "chain_as_tool = chain.as_tool(\n",
    "                              name=\"Style responder\",\n",
    "                              description=\"tool description.\"\n",
    "                             )\n",
    "\n",
    "\n",
    "chain_as_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I DON'T HAVE TIME FOR THIS STUPID CONVERSATION. WHAT DO YOU WANT?!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'answer_style': \"Angry and rude\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üßº Best practices**\n",
    "\n",
    "- Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\n",
    "- Design simple and narrowly scoped tools, as they are easier for models to use correctly.\n",
    "- Use chat models that support `tool-calling` APIs to take advantage of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **‚òéÔ∏è Calling Tools üõ†Ô∏è**\n",
    "There are cases where we want a **model to also interact directly with systems**, such as databases or an API. These systems often **have a particular input schema**; for example, APIs frequently have a required payload structure. This need motivates the concept of **tool calling**. You can use tool calling to request model responses that match a particular schema\n",
    "\n",
    "\n",
    "\n",
    "**(1) Tool Creation:** Use the `@tool` decorator to create a tool. A tool is an association between a function and its schema. \n",
    "\n",
    "**(2) Tool Binding:** The tool needs to be connected to a model that supports tool calling. **This gives the model awareness of the tool** and the associated input schema required by the tool. \n",
    "\n",
    "**(3) Tool Calling:** When appropriate, the **model can decide to call a tool** and ensure its response conforms to the tool's input schema. \n",
    "\n",
    "**(4) Tool Execution:** The tool can be executed using the arguments provided by the model.\n",
    "\n",
    "-https://python.langchain.com/docs/concepts/tool_calling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![name](https://python.langchain.com/assets/images/tool_calling_concept-552a73031228ff9144c7d59f26dedbbf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rrecommended Workflow**\n",
    "\n",
    "```python\n",
    "# Tool creation\n",
    "tools = [my_tool]\n",
    "\n",
    "# Tool binding\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Tool calling \n",
    "response = model_with_tools.invoke(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.Tool creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tool binding**\n",
    "###  The `.bind_tools()` method can be used to specify which tools are available for a model to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tool_calling_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000022610A90B50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000022610AA1510>, root_client=<openai.OpenAI object at 0x0000022610A4E0D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000022610A81110>, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'multiply', 'description': 'Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = tool_calling_model.bind_tools([multiply])\n",
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 65, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmUEOapoTSDSQ51qEjf8CYgpx790', 'finish_reason': 'stop', 'logprobs': None}, id='run-3a8a7b9d-d079-4606-8078-09af72a5f9f6-0', usage_metadata={'input_tokens': 65, 'output_tokens': 11, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"Hello world!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass an input relevant to the tool, the model should choose to call it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìû 3.Tool calling** (How to use chat models to call tools)\n",
    "- A key principle of tool calling is that the **model decides when to use a tool based on the input's relevance.**\n",
    "- The **model doesn't always need to call a tool.**\n",
    "- For example, given an **unrelated input, the model would not call the too**\n",
    "\n",
    "https://python.langchain.com/docs/how_to/tool_calling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tool_calling](https://python.langchain.com/assets/images/tool_call_example-2348b869f9a5d0d2a45dfbe614c177a4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_FIZrJ5yGnn7pe9wskNxMMghd', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 71, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmUE5ZGaJ26AStDom5ZGFYMWayxo', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2d9e8f85-6955-4c23-bd44-be09b3a195cf-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_FIZrJ5yGnn7pe9wskNxMMghd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 71, 'output_tokens': 18, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_FIZrJ5yGnn7pe9wskNxMMghd',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 65, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmUF7LMPvJeEIQ2Kxh8Iyl8zAyJs', 'finish_reason': 'stop', 'logprobs': None}, id='run-8b50d25c-341b-4cf0-81bb-3f8a3ae9f893-0', usage_metadata={'input_tokens': 65, 'output_tokens': 12, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrelated input, the model would not call the tool\n",
    "result = llm_with_tools.invoke(\"Hello world!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tool_calls is an empty list\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üö¶Attention**\n",
    "Remember, while the name `\"tool calling\"` implies that the model is directly performing some action, this is actually not the case! The **model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.**\n",
    "\n",
    "Tool calling is a general technique that **generates structured output from a model, and you can use it even when you don't intend to invoke any tools.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# tool_calling_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = init_chat_model(\"gpt-3.5-turbo\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MKt3TYZ1skF7POjl9WzVwAqr', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 78, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmUGsf7KDr5GY69xix96zsgKoJ8K', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8d095847-043a-45d2-abcc-4ba9cd1d1b3a-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_MKt3TYZ1skF7POjl9WzVwAqr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 78, 'output_tokens': 18, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1BonaEljplhtZp730dB2DXY6', 'function': {'arguments': '{\"a\": 3, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_81O4IpGeH6iKUfnViFw9sFf8', 'function': {'arguments': '{\"a\": 11, \"b\": 49}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 88, 'total_tokens': 138, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmUHKVtf0CstATpJgUmg16fZJvFF', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-89be2b2c-a00b-4169-a1ca-2a55d506888e-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_1BonaEljplhtZp730dB2DXY6', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_81O4IpGeH6iKUfnViFw9sFf8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 88, 'output_tokens': 50, 'total_tokens': 138, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "ai_msg\n",
    "# messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_1BonaEljplhtZp730dB2DXY6',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'call_81O4IpGeH6iKUfnViFw9sFf8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(ai_msg.tool_calls))\n",
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Doesn't the Tool Return `6` in the Graph?\n",
    "\n",
    "In this LangGraph example, you bind a tool (a Python function) to the LLM:\n",
    "\n",
    "```python\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "```\n",
    "Then you invoke the LLM with a message:\n",
    "\n",
    "```python\n",
    "llm_with_tools.invoke([HumanMessage(content=\"Multiply 2 and 3\")])`\n",
    "\n",
    "[{'name': 'multiply',\n",
    "  'args': {'a': 2, 'b': 3},\n",
    "  'id': 'call_P8gn6QejXtPGqLY4uXIG3W3J',\n",
    "  'type': 'tool_call'}]\n",
    "```\n",
    "## ‚ùóÔ∏è Problem: It Doesn‚Äôt Return 6\n",
    "Even though you ask the LLM to multiply 2 and 3, it does not return 6.\n",
    "\n",
    "üîç What‚Äôs Actually Happening\n",
    "The LLM suggests calling the multiply tool by returning a `ToolCall.`\n",
    "\n",
    "### But the LLM does not execute your Python function.\n",
    "\n",
    "### You need to manually detect and run the tool.\n",
    "\n",
    "‚úÖ Correct Way to Handle It\n",
    "You must:\n",
    "\n",
    "- Detect the tool call\n",
    "\n",
    "- Call the tool manually (e.g., `multiply(a=2, b=3)`)\n",
    "\n",
    "- Return the result back as a `ToolMessage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üöÄ 4. Tool Execution** (How to pass tool outputs to chat models)\n",
    "`LangGraph` offers pre-built components (e.g., `ToolNode`) that will often **invoke the tool in behalf of the user.**\n",
    "\n",
    "\n",
    " https://python.langchain.com/docs/how_to/tool_results_pass_to_model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_call: {'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_1BonaEljplhtZp730dB2DXY6', 'type': 'tool_call'}\n",
      "selected_tool: name='multiply' description='Multiplies a and b.' args_schema=<class 'langchain_core.utils.pydantic.multiply'> func=<function multiply at 0x000002260F154D60>\n",
      "tool_msg: content='36' name='multiply' tool_call_id='call_1BonaEljplhtZp730dB2DXY6'\n",
      "\n",
      "tool_call: {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_81O4IpGeH6iKUfnViFw9sFf8', 'type': 'tool_call'}\n",
      "selected_tool: name='add' description='Adds a and b.' args_schema=<class 'langchain_core.utils.pydantic.add'> func=<function add at 0x0000022610AE5120>\n",
      "tool_msg: content='60' name='add' tool_call_id='call_81O4IpGeH6iKUfnViFw9sFf8'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='call_1BonaEljplhtZp730dB2DXY6'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='call_81O4IpGeH6iKUfnViFw9sFf8')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    print(\"tool_call:\", tool_call)\n",
    "\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()] # add or multiply tools\n",
    "    print(\"selected_tool:\", selected_tool)\n",
    "\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    print(\"tool_msg:\", tool_msg)\n",
    "    \n",
    "    print()\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HumanMessage(user query) --[invokes llm_with_tools] --> AIMessage`s (might include `tool_calls`)\n",
    "- In this example `ai_msg`s includes 2 `AIMessage` each has a `tool_calls` dictionary\n",
    " \n",
    "`tool_calls --[invokes a tool]--> ToolMessage(return of fucntion execution)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that each `ToolMessage` **must include a `tool_call_id` that matches an id in the original tool calls** that the model generates. This helps the model match tool responses with tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to call tools using `ToolNode`**\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/tool-calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tool_calling_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "llm_with_tools = tool_calling_model.bind_tools([multiply])\n",
    "llm_with_tools\n",
    "\n",
    "\n",
    "# Node\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass in `Hello!`, the LLM responds without any tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%pip install grandalf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The LLM chooses to use a tool when it determines that the input or task requires the functionality provided by that tool.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_WKj7hJMsyp6Golhs9MdOERJk)\n",
      " Call ID: call_WKj7hJMsyp6Golhs9MdOERJk\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "  multiply (call_MgHjR58UPAY0akaHa1D9rRLV)\n",
      " Call ID: call_MgHjR58UPAY0akaHa1D9rRLV\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 2\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
    "\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "# Node ot manually run the tool\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    ai_msg = llm_with_tools.invoke(state[\"messages\"]) # return a tool_call object\n",
    "    print('ai_msg:', ai_msg)\n",
    "    \n",
    "    # Check if the latest message has a tool_calls\n",
    "    if ai_msg.tool_calls:\n",
    "        tool_call = ai_msg.tool_calls[0] \n",
    "\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "\n",
    "        if tool_name == \"multiply\":\n",
    "            result = multiply(**args)\n",
    "\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    ai_msg,\n",
    "                    ToolMessage(tool_call_id=tool_id, content=str(result))\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_msg: content='' additional_kwargs={'tool_calls': [{'id': 'call_9rglAd0n02LuUpYJ9e0HO9rE', 'function': {'arguments': '{\"a\": 2, \"b\": 3}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_ozefpN6JOkghJOxd5EBkwNbZ', 'function': {'arguments': '{\"a\": 3, \"b\": 2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 61, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmVfatAWTGiIciXFVMivNa7qiI08', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-64566e7a-aab0-48dd-ada1-f3ec17054779-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_9rglAd0n02LuUpYJ9e0HO9rE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_ozefpN6JOkghJOxd5EBkwNbZ', 'type': 'tool_call'}] usage_metadata={'input_tokens': 61, 'output_tokens': 50, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_9rglAd0n02LuUpYJ9e0HO9rE)\n",
      " Call ID: call_9rglAd0n02LuUpYJ9e0HO9rE\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "  multiply (call_ozefpN6JOkghJOxd5EBkwNbZ)\n",
      " Call ID: call_ozefpN6JOkghJOxd5EBkwNbZ\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
    "result = graph.invoke(input)\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_msg: content=\"I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 59, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BMmVgrrTVCmMfd8HXRO4V1VX92WVw', 'finish_reason': 'stop', 'logprobs': None} id='run-6b688d36-0e1f-4a67-92f8-f63354f3586e-0' usage_metadata={'input_tokens': 59, 'output_tokens': 38, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "how are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [HumanMessage(content=\"how are you?\")]}\n",
    "result = graph.invoke(input)\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to call tools using `ToolNode`**\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/tool-calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_QTPj2Ck39z4VWHYTRQmRK2Ha)\n",
      " Call ID: call_QTPj2Ck39z4VWHYTRQmRK2Ha\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "####### TOOL #####################\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "\n",
    "########## NODES #################\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode([multiply])\n",
    "\n",
    "\n",
    "######## GRAPH #################\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", \"tool_node\")\n",
    "builder.add_edge( \"tool_node\", END)\n",
    "\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 2 and 3\")]}\n",
    "result = graph.invoke(input)\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
