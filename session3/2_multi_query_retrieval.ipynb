{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c23a7a5-4e6c-44bb-af42-6eab64e7df70",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "\n",
    "Don't forget to run ```export OPENAI_API_KEY=sk-...``` to set your api key in the environment variables before running Jupyter. You can set up alternative api keys with hugging face or other client sites to operate with LangChain but that is beyond our scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350f594f-b754-4e79-9fc1-d8adb9934ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this cell if you need to install the required packages \n",
    "# !pip install faiss-cpu wikipedia python-certifi-win32 langchain-community langchain-openai certifi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7070d-9c10-4a52-b866-9f1270dc264d",
   "metadata": {},
   "source": [
    "# Multi-Query Retrieval: Enhancing Vector Search with LLMs\n",
    "\n",
    "## The Challenge\n",
    "In vector databases, finding the right information depends heavily on how you phrase your query. When you have thousands of documents, the exact phrasing becomes crucial - but how do you know the right phrasing when you don't know what's in your database?\n",
    "\n",
    "## The Solution: Multi-Query Retrieval\n",
    "Multi-Query Retrieval offers an elegant solution by using Large Language Models to generate multiple variations of your initial query, improving the chances of matching relevant documents.\n",
    "\n",
    "## How It Works\n",
    "1. **Query Generation**: An LLM (like ChatGPT) takes your natural question and creates several alternative phrasings\n",
    "2. **Multiple Searches**: Each generated query is used to search the vector database\n",
    "3. **Result Aggregation**: The unique results from all queries are combined to provide comprehensive information\n",
    "\n",
    "## The Notebook Implementation\n",
    "The notebook demonstrates this technique using:\n",
    "- Wikipedia data about MKUltra as sample content\n",
    "- Document splitting to create manageable chunks\n",
    "- OpenAI embeddings to vectorize the content\n",
    "- FAISS as the vector database\n",
    "- ChatGPT to generate query variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cca55-6d49-438e-8a20-5d9001bccaf7",
   "metadata": {},
   "source": [
    "## Step 1: Gathering Our Data\n",
    "\n",
    "For this example, we'll use information about Project MKUltra from Wikipedia. This CIA program was classified for many years before being declassified, making it an interesting case study for information retrieval.\n",
    "\n",
    "Let's load the data using LangChain's Wikipedia loader:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596a72d1-5984-4e7d-ac9d-90517196be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jake_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\jake_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query='MKUltra')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f267c5-94e2-4917-981d-d2d55a07124f",
   "metadata": {},
   "source": [
    "## Project MKUltra : Our Test Case\n",
    "Project MKUltra was a human experimentation program designed and undertaken by the U.S. Central Intelligence Agency (CIA) from 1953 to 1973. Its goal was to develop procedures and identify drugs that could be used during interrogations to weaken individuals and force confessions through brainwashing and psychological torture.\n",
    "\n",
    "The term MKUltra is a CIA cryptonym: \\\"MK\\\" stands for the Office of Technical Service and \\\"Ultra\\\" is an arbitrary word used to name this project. The program has been widely condemned as a violation of individual rights and an example of the CIA's abuse of power.\n",
    "\n",
    "What makes this an interesting test case for our retrieval system is that information about it was released gradually through various declassifications and investigations. This means relevant information might be phrased in various ways throughout the documents.\n",
    "   \n",
    "\n",
    "link: https://en.wikipedia.org/wiki/MKUltra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6c284f-8aea-4600-90e3-de737c438813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a4ea1-1d1f-4931-a4a0-fb8025522396",
   "metadata": {},
   "source": [
    "## Step 2: Processing Documents for Vector Storage\n",
    "\n",
    "Our Wikipedia article is quite long. For effective retrieval, we need to split it into smaller chunks that can be independently vectorized and retrieved. Think of this as creating multiple focused knowledge pieces instead of one large document.\n",
    "\n",
    "We'll use the `RecursiveCharacterTextSplitter` which intelligently splits text based on natural boundaries like paragraphs and sentences:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26b0693-0a64-42ec-8c88-a58d4a8fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ae05a7-15a3-47d3-8f11-648a09b7870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484a9899-e605-43fb-bfb5-1cd7226a5fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b51ef2-f455-45bf-83ed-7f55f4f893a2",
   "metadata": {},
   "source": [
    "We've gone from 24 document sections to 305 smaller chunks! This granularity is important for several reasons:\n",
    "\n",
    "1. **Precision**: Smaller chunks allow us to retrieve just the relevant information\n",
    "2. **Context Management**: The 100-character overlap helps maintain context between chunks\n",
    "3. **Token Efficiency**: When working with LLMs, smaller chunks help us stay within token limits\n",
    "\n",
    "The splitter tries to break at natural boundaries (paragraphs, sentences) first before resorting to character-level splits. This helps preserve the semantic integrity of our chunks.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3c21d-125c-4f8b-975a-c86451cc70cf",
   "metadata": {},
   "source": [
    "## Step 3: Creating Vector Embeddings\n",
    "\n",
    "Now we need to transform our text chunks into vector embeddings - numerical representations that capture the semantic meaning of the text. These vectors will allow us to perform similarity searches.\n",
    "\n",
    "We'll use OpenAI's embedding model for this task:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4066897f-33ff-4076-af59-56a84462fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab33fee-5e1f-465e-b6f4-84d0846f27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb9ac6-ad53-45b6-b78f-332a140dfbaa",
   "metadata": {},
   "source": [
    "## Step 4: Creating Our Vector Database\n",
    "\n",
    "Now we'll store our embeddings in a vector database. FAISS (Facebook AI Similarity Search) is a good choice because it's efficient for similarity search and doesn't require external servers:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce832928-f637-447b-ae7b-0c73281403a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b7378-2335-48cd-8f45-3290c25d6e87",
   "metadata": {},
   "source": [
    "Behind the scenes, several important things just happened:\n",
    "\n",
    "1. Each of our 305 document chunks was sent to OpenAI's embedding API\n",
    "2. The API converted each chunk into a high-dimensional vector (typically 1536 dimensions)\n",
    "3. These vectors were stored in the FAISS database\n",
    "4. FAISS created an index structure to make similarity searches efficient\n",
    "\n",
    "Now we have a searchable knowledge base that understands semantic similarity, not just keyword matching.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69514b8-2e4e-4835-bd9d-c09077545a5e",
   "metadata": {},
   "source": [
    "## Step 5: Implementing Multi-Query Retrieval\n",
    "    \n",
    "Here comes the innovative part. Instead of doing a single search with one query, we'll use an LLM to generate multiple query variations, then search for each one.\n",
    "\n",
    "First, let's import the necessary components:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0881722-eda8-4f3b-be3e-3949d48e94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928e2ad-fb56-49e1-9b07-6d217d1b686f",
   "metadata": {},
   "source": [
    "Let's frame a question that tests our system. \\\"When was this declassified?\\\" is a good example because:\n",
    "\n",
    "1. It's a natural way a person might ask\n",
    "2. It uses a pronoun (\\\"this\\\") that requires contextual understanding\n",
    "3. The answer might be phrased in various ways in the documents\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a303dd57-6183-4783-b549-22876fafd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was this declassified?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80115036-dd9e-4267-81e9-17cd1328599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use temperature=0 for consistent, deterministic outputs so the LLM is less creative\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f461f93-f8ee-4919-9cdc-e2df221a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our multi-query retriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(), llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dff90aa4-5306-4804-af46-6eeb5f9692cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to see what queries are generated\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2241d71-ee6c-4d70-8516-5d3495fa3ef1",
   "metadata": {},
   "source": [
    "Now let's execute our multi-query retrieval. This will:\n",
    "\n",
    "1. Take our original question\n",
    "2. Generate multiple variations using the LLM\n",
    "3. Search the vector database with each variation\n",
    "4. Return the combined unique results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fbed3ab-b9ff-4c87-abf3-2335ae5c5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the date of the declassification of this information?', '2. Can you provide the specific time when this was declassified?', '3. Do you know the exact moment when this information became declassified?']\n"
     ]
    }
   ],
   "source": [
    "# THIS WILL NOT DIRECTLY ANSWER ANY QUERY\n",
    "# RETURN N DOCS THAT ARE MOST SIMILAR/RELEVANT\n",
    "unique_docs = retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05886d04-6bc9-4335-a6e9-b3aaa6237eb7",
   "metadata": {},
   "source": [
    "## Understanding the Generated Queries\n",
    "Note that the underlying queries generated by the retriever are logged at the `INFO` level.\n",
    "Look at those query variations! The LLM has taken our simple question \\\"When was this declassified?\\\" and generated three different phrasings:\n",
    "\n",
    "1. \\\"What is the date of the declassification of this information?\n",
    "2. \\\"Can you provide the specific time when this was declassified?\n",
    "3. \\\"When was the classified status of this information removed?\n",
    "\n",
    "Each variation approaches the question from a slightly different angle, using different vocabulary and sentence structures. This increases our chances of matching the actual phrasing in the documents.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc83fa79-e81c-4257-a47e-0fb07acd2f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique documents did we retrieve?\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6262-3023-4130-8812-5379a91ee616",
   "metadata": {},
   "source": [
    "We've retrieved 7 unique documents across all three query variations. This is the power of multi-query retrieval - we cast a wider net and capture more potentially relevant information.\n",
    "\n",
    "Let's look at the most relevant document to see if we found our answer:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c409f85-fe32-4cc9-a24e-b28bdd35c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='. Some surviving information about MKUltra was declassified in 2001.' metadata={'title': 'MKUltra', 'summary': 'Project MKUltra was a human experimentation program designed and undertaken by the U.S. Central Intelligence Agency (CIA) to develop procedures and identify drugs that could be used during interrogations to weaken individuals and force confessions through brainwashing and psychological torture. The term MKUltra is a CIA cryptonym: \"MK\" is an arbitrary prefix standing for the Office of Technical Service and \"Ultra\" is an arbitrary word out of a dictionary used to name this project. The program has been widely condemned as a violation of individual rights and an example of the CIA’s abuse of power, with critics highlighting its disregard for consent and its corrosive impact on democratic principles.\\nProject MKUltra began in 1953 and was halted in 1973. MKUltra used numerous methods to manipulate its subjects\\' mental states and brain functions, such as the covert administration of high doses of psychoactive drugs (especially LSD) and other chemicals without the subjects\\' consent. Additionally, other methods beyond chemical compounds were used, including electroshocks, hypnosis, sensory deprivation, isolation, verbal and sexual abuse, and other forms of torture.\\nProject MKUltra was preceded by Project Artichoke. It was organized through the CIA\\'s Office of Scientific Intelligence and coordinated with the United States Army Biological Warfare Laboratories. The program engaged in illegal activities, including the use of U.S. and Canadian citizens as unwitting test subjects.:\\u200a74\\u200a MKUltra\\'s scope was broad, with activities carried out under the guise of research at more than 80 institutions aside from the military, including colleges and universities, hospitals, prisons, and pharmaceutical companies. The CIA operated using front organizations, although some top officials at these institutions were aware of the CIA\\'s involvement.\\nProject MKUltra was revealed to the public in 1975 by the Church Committee (named after Senator Frank Church) of the United States Congress and Gerald Ford\\'s United States President\\'s Commission on CIA Activities within the United States (the Rockefeller Commission). Investigative efforts were hampered by CIA Director Richard Helms\\'s order that all MKUltra files be destroyed in 1973; the Church Committee and Rockefeller Commission investigations relied on the sworn testimony of direct participants and on the small number of documents that survived Helms\\'s order. In 1977, a Freedom of Information Act request uncovered a cache of 20,000 documents relating to MKUltra, which led to Senate hearings. Some surviving information about MKUltra was declassified in 2001.', 'source': 'https://en.wikipedia.org/wiki/MKUltra'}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first (most relevant) document\n",
    "print(unique_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f95b6df-83d0-4e69-aeec-ea8687e204e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Some surviving information about MKUltra was declassified in 2001.\n"
     ]
    }
   ],
   "source": [
    "print(unique_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f40cf7-bf9d-49a0-9608-e820d16b3fd1",
   "metadata": {},
   "source": [
    "#### Supplying your own prompt\n",
    "\n",
    "Under the hood, `MultiQueryRetriever` generates queries using a specific [prompt](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html). To customize this prompt:\n",
    "\n",
    "1. Make a [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) with an input variable for the question;\n",
    "2. Implement an ```output parser``` like the one below to split the result into a list of queries.\n",
    "\n",
    "The prompt and output parser together must support the generation of a list of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68699896-cb20-4045-b929-9878fe2cfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# Other inputs\n",
    "question = \"When was this declassified?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b6c2b3-b6db-4e08-9da7-efd7c156beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the date of the declassification of this information?', '2. Can you provide the specific time when this was declassified?', '3. At what point in time was this officially made public?', '4. When did the declassification process for this occur?', '5. Do you know the exact date when this information became unclassified?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.invoke(\"When was this declassified?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee5f4c1e-ebbe-48df-80bd-3856a15e6555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Some surviving information about MKUltra was declassified in 2001.\n",
      ". NSA officials stipulated that FBI and CIA agents must destroy or return these reports within two weeks of receiving them. The NSA also required that \"the reports not be 'identified with the National Security Agency' and that all records relating to this program were 'not serialize[d]' or filed with other NSA records, were classified 'Top Secret,' and were stamped 'Background Use Only'... because the\n",
      ". The report would not be formally revealed to the public until 2007.\n",
      ". Investigative efforts were hampered by CIA Director Richard Helms's order that all MKUltra files be destroyed in 1973; the Church Committee and Rockefeller Commission investigations relied on the sworn testimony of direct participants and on the small number of documents that survived Helms's order. In 1977, a Freedom of Information Act request uncovered a cache of 20,000 documents relating to MKUltra, which led to Senate hearings\n",
      ". Then on December 22, 1974, The New York Times published a lengthy article by Seymour Hersh detailing operations engaged in by the CIA over the years that had been dubbed the \"family jewels\". Covert action programs involving assassination attempts on foreign leaders and covert attempts to subvert foreign governments were reported for the first time. In addition, the article discussed efforts by intelligence agencies to collect information on the political activities of US citizens.\n",
      "Project MKUltra was revealed to the public in 1975 by the Church Committee (named after Senator Frank Church) of the United States Congress and Gerald Ford's United States President's Commission on CIA Activities within the United States (the Rockefeller Commission)\n",
      "It also unearthed Project SHAMROCK, a program in which the major telecommunications companies shared their traffic with the NSA, and officially confirmed the existence of this signals intelligence agency to the public for the first time.\n",
      ". According to former CIA Official Cord Meyer, these disclosures \"Convinced large sections of the American public that the CIA had become a domestic Gestapo and stimulated an overwhelming demand for the wide-ranging congressional investigations that were to follow.\"\n",
      "By the early years of the 1970s, a series of troubling revelations had appeared in the press concerning intelligence activities. First came the revelations by Army intelligence officer Christopher Pyle in January 1970 of the US Army's spying on the civilian population and Senator Sam Ervin's Senate investigations produced more revelations\n"
     ]
    }
   ],
   "source": [
    "for i in unique_docs: \n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accec9b0-9b59-48d5-8a92-79f0fd57a1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
