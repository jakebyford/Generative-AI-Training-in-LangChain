{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b7070d-9c10-4a52-b866-9f1270dc264d",
   "metadata": {},
   "source": [
    "# Multi-Query Retrieval: Enhancing Vector Search with LLMs\n",
    "\n",
    "## The Challenge\n",
    "In vector databases, finding the right information depends heavily on how you phrase your query. When you have thousands of documents, the exact phrasing becomes crucial - but how do you know the right phrasing when you don't know what's in your database?\n",
    "\n",
    "## The Solution: Multi-Query Retrieval\n",
    "Multi-Query Retrieval offers an elegant solution by using Large Language Models to generate multiple variations of your initial query, improving the chances of matching relevant documents.\n",
    "\n",
    "## How It Works\n",
    "1. **Query Generation**: An LLM (like ChatGPT) takes your natural question and creates several alternative phrasings\n",
    "2. **Multiple Searches**: Each generated query is used to search the vector database\n",
    "3. **Result Aggregation**: The unique results from all queries are combined to provide comprehensive information\n",
    "\n",
    "## The Notebook Implementation\n",
    "The notebook demonstrates this technique using:\n",
    "- Wikipedia data about MKUltra as sample content\n",
    "- Document splitting to create manageable chunks\n",
    "- OpenAI embeddings to vectorize the content\n",
    "- FAISS as the vector database\n",
    "- ChatGPT to generate query variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23a7a5-4e6c-44bb-af42-6eab64e7df70",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "First, let's install the necessary libraries. We'll need:\n",
    "- `faiss-cpu` for our vector database\n",
    "- `wikipedia` for our data source\n",
    "- `langchain` components for our retrieval pipeline\n",
    "- OpenAI's API for embeddings and query generation\n",
    "\n",
    "Don't forget to run ```export OPENAI_API_KEY=sk-...``` to set your api key in the environment variables. You can set up alternative api keys with hugging face or other client sites to operate with LangChain but that is beyond our scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350f594f-b754-4e79-9fc1-d8adb9934ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this cell if you need to install the required packages \n",
    "# !pip install faiss-cpu wikipedia python-certifi-win32 langchain-community langchain-openai certifi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cca55-6d49-438e-8a20-5d9001bccaf7",
   "metadata": {},
   "source": [
    "## Step 1: Gathering Our Data\n",
    "\n",
    "For this example, we'll use information about Project MKUltra from Wikipedia. This CIA program was classified for many years before being declassified, making it an interesting case study for information retrieval.\n",
    "\n",
    "Let's load the data using LangChain's Wikipedia loader:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596a72d1-5984-4e7d-ac9d-90517196be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query='MKUltra')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f267c5-94e2-4917-981d-d2d55a07124f",
   "metadata": {},
   "source": [
    "## Project MKUltra : Our Test Case\n",
    "Project MKUltra was a human experimentation program designed and undertaken by the U.S. Central Intelligence Agency (CIA) from 1953 to 1973. Its goal was to develop procedures and identify drugs that could be used during interrogations to weaken individuals and force confessions through brainwashing and psychological torture.\n",
    "\n",
    "The term MKUltra is a CIA cryptonym: \\\"MK\\\" stands for the Office of Technical Service and \\\"Ultra\\\" is an arbitrary word used to name this project. The program has been widely condemned as a violation of individual rights and an example of the CIA's abuse of power.\n",
    "\n",
    "What makes this an interesting test case for our retrieval system is that information about it was released gradually through various declassifications and investigations. This means relevant information might be phrased in various ways throughout the documents.\n",
    "   \n",
    "\n",
    "link: https://en.wikipedia.org/wiki/MKUltra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6c284f-8aea-4600-90e3-de737c438813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a4ea1-1d1f-4931-a4a0-fb8025522396",
   "metadata": {},
   "source": [
    "## Step 2: Processing Documents for Vector Storage\n",
    "\n",
    "Our Wikipedia article is quite long. For effective retrieval, we need to split it into smaller chunks that can be independently vectorized and retrieved. Think of this as creating multiple focused knowledge pieces instead of one large document.\n",
    "\n",
    "We'll use the `RecursiveCharacterTextSplitter` which intelligently splits text based on natural boundaries like paragraphs and sentences:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26b0693-0a64-42ec-8c88-a58d4a8fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ae05a7-15a3-47d3-8f11-648a09b7870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484a9899-e605-43fb-bfb5-1cd7226a5fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b51ef2-f455-45bf-83ed-7f55f4f893a2",
   "metadata": {},
   "source": [
    "We've gone from 24 document sections to 305 smaller chunks! This granularity is important for several reasons:\n",
    "\n",
    "1. **Precision**: Smaller chunks allow us to retrieve just the relevant information\n",
    "2. **Context Management**: The 100-character overlap helps maintain context between chunks\n",
    "3. **Token Efficiency**: When working with LLMs, smaller chunks help us stay within token limits\n",
    "\n",
    "The splitter tries to break at natural boundaries (paragraphs, sentences) first before resorting to character-level splits. This helps preserve the semantic integrity of our chunks.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3c21d-125c-4f8b-975a-c86451cc70cf",
   "metadata": {},
   "source": [
    "## Step 3: Creating Vector Embeddings\n",
    "\n",
    "Now we need to transform our text chunks into vector embeddings - numerical representations that capture the semantic meaning of the text. These vectors will allow us to perform similarity searches.\n",
    "\n",
    "We'll use OpenAI's embedding model for this task:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4066897f-33ff-4076-af59-56a84462fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab33fee-5e1f-465e-b6f4-84d0846f27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb9ac6-ad53-45b6-b78f-332a140dfbaa",
   "metadata": {},
   "source": [
    "## Step 4: Creating Our Vector Database\n",
    "\n",
    "Now we'll store our embeddings in a vector database. FAISS (Facebook AI Similarity Search) is a good choice because it's efficient for similarity search and doesn't require external servers:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce832928-f637-447b-ae7b-0c73281403a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b7378-2335-48cd-8f45-3290c25d6e87",
   "metadata": {},
   "source": [
    "Behind the scenes, several important things just happened:\n",
    "\n",
    "1. Each of our 305 document chunks was sent to OpenAI's embedding API\n",
    "2. The API converted each chunk into a high-dimensional vector (typically 1536 dimensions)\n",
    "3. These vectors were stored in the FAISS database\n",
    "4. FAISS created an index structure to make similarity searches efficient\n",
    "\n",
    "Now we have a searchable knowledge base that understands semantic similarity, not just keyword matching.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69514b8-2e4e-4835-bd9d-c09077545a5e",
   "metadata": {},
   "source": [
    "## Step 5: Implementing Multi-Query Retrieval\n",
    "    \n",
    "Here comes the innovative part. Instead of doing a single search with one query, we'll use an LLM to generate multiple query variations, then search for each one.\n",
    "\n",
    "First, let's import the necessary components:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0881722-eda8-4f3b-be3e-3949d48e94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928e2ad-fb56-49e1-9b07-6d217d1b686f",
   "metadata": {},
   "source": [
    "Let's frame a question that tests our system. \\\"When was this declassified?\\\" is a good example because:\n",
    "\n",
    "1. It's a natural way a person might ask\n",
    "2. It uses a pronoun (\\\"this\\\") that requires contextual understanding\n",
    "3. The answer might be phrased in various ways in the documents\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a303dd57-6183-4783-b549-22876fafd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was this declassified?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80115036-dd9e-4267-81e9-17cd1328599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use temperature=0 for consistent, deterministic outputs so the LLM is less creative\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f461f93-f8ee-4919-9cdc-e2df221a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our multi-query retriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(), llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dff90aa4-5306-4804-af46-6eeb5f9692cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to see what queries are generated\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2241d71-ee6c-4d70-8516-5d3495fa3ef1",
   "metadata": {},
   "source": [
    "Now let's execute our multi-query retrieval. This will:\n",
    "\n",
    "1. Take our original question\n",
    "2. Generate multiple variations using the LLM\n",
    "3. Search the vector database with each variation\n",
    "4. Return the combined unique results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fbed3ab-b9ff-4c87-abf3-2335ae5c5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the date of the declassification of this information?', '2. Can you provide the specific time when this was declassified?', '3. When was the classified status of this information removed?']\n"
     ]
    }
   ],
   "source": [
    "# THIS WILL NOT DIRECTLY ANSWER ANY QUERY\n",
    "# RETURN N DOCS THAT ARE MOST SIMILAR/RELEVANT\n",
    "unique_docs = retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05886d04-6bc9-4335-a6e9-b3aaa6237eb7",
   "metadata": {},
   "source": [
    "## Understanding the Generated Queries\n",
    "\n",
    "Look at those query variations! The LLM has taken our simple question \\\"When was this declassified?\\\" and generated three different phrasings:\n",
    "\n",
    "1. \\\"What is the date of the declassification of this information?\n",
    "2. \\\"Can you provide the specific time when this was declassified?\n",
    "3. \\\"When was the classified status of this information removed?\n",
    "\n",
    "Each variation approaches the question from a slightly different angle, using different vocabulary and sentence structures. This increases our chances of matching the actual phrasing in the documents.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc83fa79-e81c-4257-a47e-0fb07acd2f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique documents did we retrieve?\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6262-3023-4130-8812-5379a91ee616",
   "metadata": {},
   "source": [
    "We've retrieved 7 unique documents across all three query variations. This is the power of multi-query retrieval - we cast a wider net and capture more potentially relevant information.\n",
    "\n",
    "Let's look at the most relevant document to see if we found our answer:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c409f85-fe32-4cc9-a24e-b28bdd35c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='. Some surviving information about MKUltra was declassified in 2001.' metadata={'title': 'MKUltra', 'summary': 'Project MKUltra was a human experimentation program designed and undertaken by the U.S. Central Intelligence Agency (CIA) to develop procedures and identify drugs that could be used during interrogations to weaken individuals and force confessions through brainwashing and psychological torture. The term MKUltra is a CIA cryptonym: \"MK\" is an arbitrary prefix standing for the Office of Technical Service and \"Ultra\" is an arbitrary word out of a dictionary used to name this project. The program has been widely condemned as a violation of individual rights and an example of the CIA’s abuse of power, with critics highlighting its disregard for consent and its corrosive impact on democratic principles.\\nProject MKUltra began in 1953 and was halted in 1973. MKUltra used numerous methods to manipulate its subjects\\' mental states and brain functions, such as the covert administration of high doses of psychoactive drugs (especially LSD) and other chemicals without the subjects\\' consent. Additionally, other methods beyond chemical compounds were used, including electroshocks, hypnosis, sensory deprivation, isolation, verbal and sexual abuse, and other forms of torture.\\nProject MKUltra was preceded by Project Artichoke. It was organized through the CIA\\'s Office of Scientific Intelligence and coordinated with the United States Army Biological Warfare Laboratories. The program engaged in illegal activities, including the use of U.S. and Canadian citizens as unwitting test subjects.:\\u200a74\\u200a MKUltra\\'s scope was broad, with activities carried out under the guise of research at more than 80 institutions aside from the military, including colleges and universities, hospitals, prisons, and pharmaceutical companies. The CIA operated using front organizations, although some top officials at these institutions were aware of the CIA\\'s involvement.\\nProject MKUltra was revealed to the public in 1975 by the Church Committee (named after Senator Frank Church) of the United States Congress and Gerald Ford\\'s United States President\\'s Commission on CIA Activities within the United States (the Rockefeller Commission). Investigative efforts were hampered by CIA Director Richard Helms\\'s order that all MKUltra files be destroyed in 1973; the Church Committee and Rockefeller Commission investigations relied on the sworn testimony of direct participants and on the small number of documents that survived Helms\\'s order. In 1977, a Freedom of Information Act request uncovered a cache of 20,000 documents relating to MKUltra, which led to Senate hearings. Some surviving information about MKUltra was declassified in 2001.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/MKUltra'}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first (most relevant) document\n",
    "print(unique_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f95b6df-83d0-4e69-aeec-ea8687e204e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Some surviving information about MKUltra was declassified in 2001.\n"
     ]
    }
   ],
   "source": [
    "print(unique_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d6ebe-b42a-431e-9a27-a9f4030353bf",
   "metadata": {},
   "source": [
    "## Success! Finding Our Answer\n",
    "\n",
    "Our multi-query approach worked perfectly! We found the exact information we were looking for:\n",
    "\n",
    "\\\"Some surviving information about MKUltra was declassified in 2001.\\\"\n",
    "\n",
    "\n",
    "## Key Takeaways about Multi-Query Retrieval\n",
    "\n",
    "Let's reflect on what makes this approach powerful:\n",
    "\n",
    "1. **Natural User Experience**: Users can ask questions in their own words without knowing the exact phrasing in the documents\n",
    "\n",
    "2. **Increased Coverage**: By generating multiple query variations, we cast a wider net in our semantic search\n",
    "\n",
    "3. **Contextual Understanding**: The LLM understands that \\\"this\\\" in our question refers to MKUltra and generates appropriate variations\n",
    "\n",
    "4. **Efficient Implementation**: The entire process requires minimal code once the basic vector retrieval system is in place\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "This technique can be applied to many real-world scenarios:\n",
    "- **Customer Support**: Helping customers find answers without knowing the exact terms used in support documentation\n",
    "- **Research Assistance**: Finding relevant papers and documents when exploring new fields\n",
    "- **Knowledge Management**: Making corporate knowledge bases more accessible to employees\n",
    "- **Educational Tools**: Helping students find relevant information in learning materials\n",
    "\n",
    "Multi-query retrieval represents an important evolution in how we interact with large knowledge bases, making information retrieval more intuitive and human-centered.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbba15-696e-4021-9ceb-85de775b6073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
